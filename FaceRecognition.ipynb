{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiralikadyrov/datasciencecoursera/blob/master/FaceRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HlPCAc27iRJ",
        "outputId": "16a8967d-b29a-473a-b3e8-63d59473baa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62\n",
            "62\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 125, 94, 3)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 131, 100, 3)  0           ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 63, 47, 64)   9472        ['conv1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 63, 47, 64)   256         ['conv1_conv[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 63, 47, 64)   0           ['conv1_bn[0][0]']               \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 65, 49, 64)   0           ['conv1_relu[0][0]']             \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 32, 24, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 32, 24, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 32, 24, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 32, 24, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 32, 24, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 32, 24, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 32, 24, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 32, 24, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 32, 24, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 32, 24, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 32, 24, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 32, 24, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 32, 24, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 32, 24, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 32, 24, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 32, 24, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 32, 24, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 32, 24, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 32, 24, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 32, 24, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 32, 24, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 32, 24, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 32, 24, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 32, 24, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 32, 24, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 32, 24, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 32, 24, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 32, 24, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 32, 24, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 32, 24, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 32, 24, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 32, 24, 256)  0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 32, 24, 256)  0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 16, 12, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 16, 12, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 16, 12, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 16, 12, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 16, 12, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 16, 12, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 16, 12, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 16, 12, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 16, 12, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 16, 12, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 16, 12, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 16, 12, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 16, 12, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 16, 12, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 16, 12, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 16, 12, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 16, 12, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 16, 12, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 16, 12, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 16, 12, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 16, 12, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 16, 12, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 16, 12, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 16, 12, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 16, 12, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 16, 12, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 16, 12, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 16, 12, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 16, 12, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 16, 12, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 16, 12, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 16, 12, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 16, 12, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 16, 12, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 16, 12, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 16, 12, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 16, 12, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 16, 12, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 16, 12, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 16, 12, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 16, 12, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 16, 12, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 8, 6, 256)    131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 8, 6, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 8, 6, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 8, 6, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 8, 6, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                                                  'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 8, 6, 1024)   0           ['conv4_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 8, 6, 256)    262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 8, 6, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 8, 6, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 8, 6, 1024)   0           ['conv4_block1_out[0][0]',       \n",
            "                                                                  'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 8, 6, 1024)   0           ['conv4_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 8, 6, 256)    262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 8, 6, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 8, 6, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 8, 6, 1024)   0           ['conv4_block2_out[0][0]',       \n",
            "                                                                  'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 8, 6, 1024)   0           ['conv4_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 8, 6, 256)    262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 8, 6, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 8, 6, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 8, 6, 1024)   0           ['conv4_block3_out[0][0]',       \n",
            "                                                                  'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 8, 6, 1024)   0           ['conv4_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 8, 6, 256)    262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 8, 6, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 8, 6, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 8, 6, 1024)   0           ['conv4_block4_out[0][0]',       \n",
            "                                                                  'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 8, 6, 1024)   0           ['conv4_block5_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 8, 6, 256)    262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 8, 6, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 8, 6, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 8, 6, 1024)   0           ['conv4_block5_out[0][0]',       \n",
            "                                                                  'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 8, 6, 1024)   0           ['conv4_block6_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block7_1_conv (Conv2D)   (None, 8, 6, 256)    262400      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block7_1_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block7_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block7_1_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block7_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block7_2_conv (Conv2D)   (None, 8, 6, 256)    590080      ['conv4_block7_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block7_2_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block7_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block7_2_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block7_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block7_3_conv (Conv2D)   (None, 8, 6, 1024)   263168      ['conv4_block7_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block7_3_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block7_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block7_add (Add)         (None, 8, 6, 1024)   0           ['conv4_block6_out[0][0]',       \n",
            "                                                                  'conv4_block7_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block7_out (Activation)  (None, 8, 6, 1024)   0           ['conv4_block7_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block8_1_conv (Conv2D)   (None, 8, 6, 256)    262400      ['conv4_block7_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block8_1_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block8_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block8_1_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block8_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block8_2_conv (Conv2D)   (None, 8, 6, 256)    590080      ['conv4_block8_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block8_2_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block8_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block8_2_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block8_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block8_3_conv (Conv2D)   (None, 8, 6, 1024)   263168      ['conv4_block8_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block8_3_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block8_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block8_add (Add)         (None, 8, 6, 1024)   0           ['conv4_block7_out[0][0]',       \n",
            "                                                                  'conv4_block8_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block8_out (Activation)  (None, 8, 6, 1024)   0           ['conv4_block8_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block9_1_conv (Conv2D)   (None, 8, 6, 256)    262400      ['conv4_block8_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block9_1_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block9_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block9_1_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block9_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block9_2_conv (Conv2D)   (None, 8, 6, 256)    590080      ['conv4_block9_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block9_2_bn (BatchNormal  (None, 8, 6, 256)   1024        ['conv4_block9_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block9_2_relu (Activatio  (None, 8, 6, 256)   0           ['conv4_block9_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block9_3_conv (Conv2D)   (None, 8, 6, 1024)   263168      ['conv4_block9_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block9_3_bn (BatchNormal  (None, 8, 6, 1024)  4096        ['conv4_block9_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block9_add (Add)         (None, 8, 6, 1024)   0           ['conv4_block8_out[0][0]',       \n",
            "                                                                  'conv4_block9_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block9_out (Activation)  (None, 8, 6, 1024)   0           ['conv4_block9_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block10_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block9_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block10_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block10_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block10_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block10_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block10_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block10_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block10_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block10_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block10_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block10_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block10_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block10_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block10_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block10_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block10_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block9_out[0][0]',       \n",
            "                                                                  'conv4_block10_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block10_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block10_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block11_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block10_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block11_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block11_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block11_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block11_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block11_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block11_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block11_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block11_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block11_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block11_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block11_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block11_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block11_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block11_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block11_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block10_out[0][0]',      \n",
            "                                                                  'conv4_block11_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block11_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block11_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block12_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block11_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block12_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block12_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block12_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block12_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block12_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block12_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block12_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block12_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block12_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block12_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block12_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block12_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block12_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block12_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block12_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block11_out[0][0]',      \n",
            "                                                                  'conv4_block12_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block12_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block12_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block13_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block12_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block13_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block13_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block13_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block13_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block13_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block13_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block13_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block13_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block13_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block13_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block13_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block13_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block13_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block13_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block13_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block12_out[0][0]',      \n",
            "                                                                  'conv4_block13_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block13_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block13_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block14_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block13_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block14_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block14_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block14_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block14_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block14_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block14_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block14_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block14_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block14_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block14_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block14_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block14_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block14_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block14_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block14_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block13_out[0][0]',      \n",
            "                                                                  'conv4_block14_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block14_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block14_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block15_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block14_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block15_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block15_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block15_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block15_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block15_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block15_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block15_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block15_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block15_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block15_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block15_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block15_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block15_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block15_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block15_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block14_out[0][0]',      \n",
            "                                                                  'conv4_block15_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block15_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block15_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block16_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block15_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block16_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block16_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block16_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block16_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block16_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block16_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block16_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block16_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block16_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block16_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block16_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block16_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block16_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block16_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block16_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block15_out[0][0]',      \n",
            "                                                                  'conv4_block16_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block16_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block16_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block17_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block16_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block17_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block17_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block17_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block17_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block17_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block17_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block17_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block17_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block17_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block17_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block17_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block17_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block17_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block17_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block17_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block16_out[0][0]',      \n",
            "                                                                  'conv4_block17_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block17_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block17_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block18_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block17_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block18_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block18_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block18_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block18_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block18_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block18_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block18_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block18_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block18_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block18_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block18_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block18_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block18_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block18_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block18_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block17_out[0][0]',      \n",
            "                                                                  'conv4_block18_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block18_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block18_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block19_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block18_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block19_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block19_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block19_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block19_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block19_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block19_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block19_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block19_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block19_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block19_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block19_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block19_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block19_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block19_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block19_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block18_out[0][0]',      \n",
            "                                                                  'conv4_block19_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block19_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block19_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block20_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block19_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block20_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block20_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block20_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block20_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block20_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block20_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block20_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block20_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block20_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block20_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block20_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block20_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block20_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block20_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block20_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block19_out[0][0]',      \n",
            "                                                                  'conv4_block20_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block20_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block20_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block21_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block20_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block21_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block21_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block21_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block21_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block21_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block21_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block21_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block21_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block21_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block21_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block21_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block21_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block21_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block21_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block21_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block20_out[0][0]',      \n",
            "                                                                  'conv4_block21_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block21_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block21_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block22_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block21_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block22_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block22_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block22_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block22_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block22_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block22_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block22_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block22_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block22_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block22_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block22_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block22_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block22_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block22_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block22_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block21_out[0][0]',      \n",
            "                                                                  'conv4_block22_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block22_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block22_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block23_1_conv (Conv2D)  (None, 8, 6, 256)    262400      ['conv4_block22_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block23_1_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block23_1_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block23_1_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block23_1_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block23_2_conv (Conv2D)  (None, 8, 6, 256)    590080      ['conv4_block23_1_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block23_2_bn (BatchNorma  (None, 8, 6, 256)   1024        ['conv4_block23_2_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block23_2_relu (Activati  (None, 8, 6, 256)   0           ['conv4_block23_2_bn[0][0]']     \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block23_3_conv (Conv2D)  (None, 8, 6, 1024)   263168      ['conv4_block23_2_relu[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block23_3_bn (BatchNorma  (None, 8, 6, 1024)  4096        ['conv4_block23_3_conv[0][0]']   \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv4_block23_add (Add)        (None, 8, 6, 1024)   0           ['conv4_block22_out[0][0]',      \n",
            "                                                                  'conv4_block23_3_bn[0][0]']     \n",
            "                                                                                                  \n",
            " conv4_block23_out (Activation)  (None, 8, 6, 1024)  0           ['conv4_block23_add[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 4, 3, 512)    524800      ['conv4_block23_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 4, 3, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 4, 3, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 4, 3, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 4, 3, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 4, 3, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 4, 3, 2048)   2099200     ['conv4_block23_out[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 4, 3, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 4, 3, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 4, 3, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 4, 3, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                                                  'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 4, 3, 2048)   0           ['conv5_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 4, 3, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 4, 3, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 4, 3, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 4, 3, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 4, 3, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 4, 3, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 4, 3, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 4, 3, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 4, 3, 2048)   0           ['conv5_block1_out[0][0]',       \n",
            "                                                                  'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 4, 3, 2048)   0           ['conv5_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 4, 3, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 4, 3, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 4, 3, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 4, 3, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 4, 3, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 4, 3, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 4, 3, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 4, 3, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 4, 3, 2048)   0           ['conv5_block2_out[0][0]',       \n",
            "                                                                  'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 4, 3, 2048)   0           ['conv5_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " global_average_pooling2d_5 (Gl  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 128)          262272      ['global_average_pooling2d_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 62)           7998        ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 42,928,446\n",
            "Trainable params: 270,270\n",
            "Non-trainable params: 42,658,176\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications.resnet import ResNet101, preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the LFW dataset and prepare the data\n",
        "# Assume you have your own code to load the LFW dataset\n",
        "faces= fetch_lfw_people(min_faces_per_person=20 ,color = True, resize = None)\n",
        "X,y = faces.images, faces.target\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,stratify = y)\n",
        "\n",
        "# Preprocess the images\n",
        "X_train = preprocess_input(X_train)\n",
        "X_test = preprocess_input(X_test)\n",
        "\n",
        "print(len(set(y_test)))\n",
        "print(len(set(y_train)))\n",
        "# Convert labels to one-hot encoded vectors\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_train = to_categorical(y_train)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "y_test = label_encoder.fit_transform(y_test)\n",
        "y_test = to_categorical(y_test)\n",
        "# Load the pre-trained ResNet50 model without the top classification layer\n",
        "base_model = ResNet101(weights='imagenet', include_top=False, input_shape=(125, 94,3))\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add your own classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "#x = Dense(512, activation='relu')(x)\n",
        "#x = Dense(256, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "#x = Dense(32, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6bU8GdUMTPg",
        "outputId": "a74214cb-8619-4dc2-85bf-ae42932120e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "85/85 [==============================] - 13s 85ms/step - loss: 3.7561 - accuracy: 0.1662 - val_loss: 3.6783 - val_accuracy: 0.1749\n",
            "Epoch 2/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 3.6887 - accuracy: 0.1754 - val_loss: 3.6654 - val_accuracy: 0.1749\n",
            "Epoch 3/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 3.6837 - accuracy: 0.1754 - val_loss: 3.6570 - val_accuracy: 0.1749\n",
            "Epoch 4/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.6742 - accuracy: 0.1754 - val_loss: 3.6563 - val_accuracy: 0.1749\n",
            "Epoch 5/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 3.6665 - accuracy: 0.1754 - val_loss: 3.6407 - val_accuracy: 0.1749\n",
            "Epoch 6/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 3.6529 - accuracy: 0.1754 - val_loss: 3.6452 - val_accuracy: 0.1749\n",
            "Epoch 7/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 3.6476 - accuracy: 0.1754 - val_loss: 3.6260 - val_accuracy: 0.1749\n",
            "Epoch 8/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.6336 - accuracy: 0.1776 - val_loss: 3.6204 - val_accuracy: 0.1749\n",
            "Epoch 9/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.6301 - accuracy: 0.1746 - val_loss: 3.6118 - val_accuracy: 0.1749\n",
            "Epoch 10/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.6142 - accuracy: 0.1754 - val_loss: 3.6005 - val_accuracy: 0.1749\n",
            "Epoch 11/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.6061 - accuracy: 0.1754 - val_loss: 3.5894 - val_accuracy: 0.1749\n",
            "Epoch 12/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 3.5897 - accuracy: 0.1754 - val_loss: 3.5789 - val_accuracy: 0.1749\n",
            "Epoch 13/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 3.5770 - accuracy: 0.1757 - val_loss: 3.5658 - val_accuracy: 0.1749\n",
            "Epoch 14/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.5646 - accuracy: 0.1842 - val_loss: 3.5716 - val_accuracy: 0.1749\n",
            "Epoch 15/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.5514 - accuracy: 0.1827 - val_loss: 3.5385 - val_accuracy: 0.1914\n",
            "Epoch 16/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.5343 - accuracy: 0.1842 - val_loss: 3.5226 - val_accuracy: 0.1848\n",
            "Epoch 17/500\n",
            "85/85 [==============================] - 4s 53ms/step - loss: 3.5233 - accuracy: 0.1827 - val_loss: 3.5197 - val_accuracy: 0.2112\n",
            "Epoch 18/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.5125 - accuracy: 0.1923 - val_loss: 3.5000 - val_accuracy: 0.2112\n",
            "Epoch 19/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.4964 - accuracy: 0.1912 - val_loss: 3.4943 - val_accuracy: 0.2079\n",
            "Epoch 20/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 3.4786 - accuracy: 0.2000 - val_loss: 3.4777 - val_accuracy: 0.2145\n",
            "Epoch 21/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 3.4655 - accuracy: 0.1989 - val_loss: 3.4600 - val_accuracy: 0.2046\n",
            "Epoch 22/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 3.4533 - accuracy: 0.2074 - val_loss: 3.4442 - val_accuracy: 0.2112\n",
            "Epoch 23/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 3.4433 - accuracy: 0.2077 - val_loss: 3.4676 - val_accuracy: 0.1848\n",
            "Epoch 24/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 3.4232 - accuracy: 0.2107 - val_loss: 3.4144 - val_accuracy: 0.2145\n",
            "Epoch 25/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.4021 - accuracy: 0.2191 - val_loss: 3.4224 - val_accuracy: 0.2013\n",
            "Epoch 26/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.3923 - accuracy: 0.2136 - val_loss: 3.3844 - val_accuracy: 0.2310\n",
            "Epoch 27/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.3707 - accuracy: 0.2235 - val_loss: 3.3841 - val_accuracy: 0.2145\n",
            "Epoch 28/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 3.3586 - accuracy: 0.2290 - val_loss: 3.3675 - val_accuracy: 0.2310\n",
            "Epoch 29/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.3402 - accuracy: 0.2232 - val_loss: 3.3568 - val_accuracy: 0.2211\n",
            "Epoch 30/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 3.3289 - accuracy: 0.2221 - val_loss: 3.3294 - val_accuracy: 0.2409\n",
            "Epoch 31/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.3106 - accuracy: 0.2243 - val_loss: 3.3239 - val_accuracy: 0.2343\n",
            "Epoch 32/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 3.3019 - accuracy: 0.2276 - val_loss: 3.3203 - val_accuracy: 0.2178\n",
            "Epoch 33/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.2819 - accuracy: 0.2239 - val_loss: 3.3107 - val_accuracy: 0.2376\n",
            "Epoch 34/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 3.2651 - accuracy: 0.2279 - val_loss: 3.2967 - val_accuracy: 0.2376\n",
            "Epoch 35/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 3.2489 - accuracy: 0.2349 - val_loss: 3.2684 - val_accuracy: 0.2310\n",
            "Epoch 36/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.2359 - accuracy: 0.2397 - val_loss: 3.2667 - val_accuracy: 0.2475\n",
            "Epoch 37/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 3.2134 - accuracy: 0.2408 - val_loss: 3.2775 - val_accuracy: 0.2211\n",
            "Epoch 38/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.1989 - accuracy: 0.2463 - val_loss: 3.2320 - val_accuracy: 0.2376\n",
            "Epoch 39/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 3.1861 - accuracy: 0.2438 - val_loss: 3.2231 - val_accuracy: 0.2508\n",
            "Epoch 40/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 3.1672 - accuracy: 0.2449 - val_loss: 3.2261 - val_accuracy: 0.2277\n",
            "Epoch 41/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.1595 - accuracy: 0.2452 - val_loss: 3.2016 - val_accuracy: 0.2475\n",
            "Epoch 42/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.1448 - accuracy: 0.2529 - val_loss: 3.2332 - val_accuracy: 0.2376\n",
            "Epoch 43/500\n",
            "85/85 [==============================] - 4s 53ms/step - loss: 3.1231 - accuracy: 0.2548 - val_loss: 3.1734 - val_accuracy: 0.2409\n",
            "Epoch 44/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 3.1174 - accuracy: 0.2496 - val_loss: 3.1714 - val_accuracy: 0.2673\n",
            "Epoch 45/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.1062 - accuracy: 0.2507 - val_loss: 3.1758 - val_accuracy: 0.2640\n",
            "Epoch 46/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 3.0803 - accuracy: 0.2533 - val_loss: 3.1512 - val_accuracy: 0.2640\n",
            "Epoch 47/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.0785 - accuracy: 0.2596 - val_loss: 3.1337 - val_accuracy: 0.2475\n",
            "Epoch 48/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.0689 - accuracy: 0.2555 - val_loss: 3.1230 - val_accuracy: 0.2574\n",
            "Epoch 49/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.0451 - accuracy: 0.2618 - val_loss: 3.1136 - val_accuracy: 0.2673\n",
            "Epoch 50/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.0293 - accuracy: 0.2640 - val_loss: 3.1054 - val_accuracy: 0.2607\n",
            "Epoch 51/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.0199 - accuracy: 0.2643 - val_loss: 3.1030 - val_accuracy: 0.2640\n",
            "Epoch 52/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 3.0152 - accuracy: 0.2654 - val_loss: 3.1504 - val_accuracy: 0.2442\n",
            "Epoch 53/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 3.0038 - accuracy: 0.2728 - val_loss: 3.0731 - val_accuracy: 0.2673\n",
            "Epoch 54/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.9890 - accuracy: 0.2688 - val_loss: 3.0714 - val_accuracy: 0.2574\n",
            "Epoch 55/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.9702 - accuracy: 0.2691 - val_loss: 3.0689 - val_accuracy: 0.2739\n",
            "Epoch 56/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 2.9738 - accuracy: 0.2684 - val_loss: 3.0699 - val_accuracy: 0.2640\n",
            "Epoch 57/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.9618 - accuracy: 0.2743 - val_loss: 3.0526 - val_accuracy: 0.2574\n",
            "Epoch 58/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.9433 - accuracy: 0.2746 - val_loss: 3.0488 - val_accuracy: 0.2805\n",
            "Epoch 59/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.9369 - accuracy: 0.2772 - val_loss: 3.0296 - val_accuracy: 0.2673\n",
            "Epoch 60/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.9183 - accuracy: 0.2743 - val_loss: 3.0103 - val_accuracy: 0.2739\n",
            "Epoch 61/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.9012 - accuracy: 0.2779 - val_loss: 3.0041 - val_accuracy: 0.2838\n",
            "Epoch 62/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 2.8948 - accuracy: 0.2886 - val_loss: 3.0048 - val_accuracy: 0.2805\n",
            "Epoch 63/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.8812 - accuracy: 0.2864 - val_loss: 2.9972 - val_accuracy: 0.2673\n",
            "Epoch 64/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 2.8714 - accuracy: 0.2790 - val_loss: 2.9998 - val_accuracy: 0.2673\n",
            "Epoch 65/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.8589 - accuracy: 0.2857 - val_loss: 3.0124 - val_accuracy: 0.3036\n",
            "Epoch 66/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.8573 - accuracy: 0.2857 - val_loss: 3.0001 - val_accuracy: 0.2640\n",
            "Epoch 67/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.8446 - accuracy: 0.2930 - val_loss: 2.9899 - val_accuracy: 0.2772\n",
            "Epoch 68/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.8397 - accuracy: 0.2949 - val_loss: 2.9773 - val_accuracy: 0.2706\n",
            "Epoch 69/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.8251 - accuracy: 0.2985 - val_loss: 2.9510 - val_accuracy: 0.2904\n",
            "Epoch 70/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.8121 - accuracy: 0.2945 - val_loss: 2.9489 - val_accuracy: 0.2838\n",
            "Epoch 71/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.7998 - accuracy: 0.3029 - val_loss: 2.9423 - val_accuracy: 0.2970\n",
            "Epoch 72/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.7989 - accuracy: 0.2960 - val_loss: 2.9151 - val_accuracy: 0.2937\n",
            "Epoch 73/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.7826 - accuracy: 0.3018 - val_loss: 2.9396 - val_accuracy: 0.2772\n",
            "Epoch 74/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.7796 - accuracy: 0.3015 - val_loss: 2.9453 - val_accuracy: 0.3036\n",
            "Epoch 75/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.7690 - accuracy: 0.3000 - val_loss: 2.9118 - val_accuracy: 0.3003\n",
            "Epoch 76/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.7540 - accuracy: 0.3081 - val_loss: 2.9034 - val_accuracy: 0.3168\n",
            "Epoch 77/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.7468 - accuracy: 0.3092 - val_loss: 2.9144 - val_accuracy: 0.3069\n",
            "Epoch 78/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.7534 - accuracy: 0.3066 - val_loss: 2.8977 - val_accuracy: 0.2937\n",
            "Epoch 79/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.7379 - accuracy: 0.3118 - val_loss: 2.9209 - val_accuracy: 0.2904\n",
            "Epoch 80/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.7313 - accuracy: 0.3191 - val_loss: 2.8915 - val_accuracy: 0.3036\n",
            "Epoch 81/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.7325 - accuracy: 0.3103 - val_loss: 2.8821 - val_accuracy: 0.2904\n",
            "Epoch 82/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.7219 - accuracy: 0.3162 - val_loss: 2.8526 - val_accuracy: 0.3003\n",
            "Epoch 83/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.7013 - accuracy: 0.3176 - val_loss: 2.8565 - val_accuracy: 0.3036\n",
            "Epoch 84/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.7062 - accuracy: 0.3206 - val_loss: 2.8546 - val_accuracy: 0.3036\n",
            "Epoch 85/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.6924 - accuracy: 0.3221 - val_loss: 2.8799 - val_accuracy: 0.3069\n",
            "Epoch 86/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.6816 - accuracy: 0.3206 - val_loss: 2.8411 - val_accuracy: 0.3135\n",
            "Epoch 87/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.6792 - accuracy: 0.3221 - val_loss: 2.9168 - val_accuracy: 0.2838\n",
            "Epoch 88/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 2.6947 - accuracy: 0.3191 - val_loss: 2.9045 - val_accuracy: 0.3036\n",
            "Epoch 89/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.6705 - accuracy: 0.3232 - val_loss: 2.8251 - val_accuracy: 0.3135\n",
            "Epoch 90/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.6604 - accuracy: 0.3324 - val_loss: 2.8518 - val_accuracy: 0.3003\n",
            "Epoch 91/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.6400 - accuracy: 0.3265 - val_loss: 2.8165 - val_accuracy: 0.2970\n",
            "Epoch 92/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.6450 - accuracy: 0.3324 - val_loss: 2.8214 - val_accuracy: 0.3036\n",
            "Epoch 93/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.6281 - accuracy: 0.3364 - val_loss: 2.8388 - val_accuracy: 0.2904\n",
            "Epoch 94/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.6157 - accuracy: 0.3393 - val_loss: 2.8178 - val_accuracy: 0.3069\n",
            "Epoch 95/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.6260 - accuracy: 0.3320 - val_loss: 2.8562 - val_accuracy: 0.2871\n",
            "Epoch 96/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.6015 - accuracy: 0.3401 - val_loss: 2.7674 - val_accuracy: 0.3036\n",
            "Epoch 97/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.6005 - accuracy: 0.3393 - val_loss: 2.7808 - val_accuracy: 0.3168\n",
            "Epoch 98/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.5991 - accuracy: 0.3364 - val_loss: 2.7775 - val_accuracy: 0.3432\n",
            "Epoch 99/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.5890 - accuracy: 0.3404 - val_loss: 2.7667 - val_accuracy: 0.3135\n",
            "Epoch 100/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.5847 - accuracy: 0.3401 - val_loss: 2.7908 - val_accuracy: 0.3234\n",
            "Epoch 101/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.5813 - accuracy: 0.3482 - val_loss: 2.7916 - val_accuracy: 0.2937\n",
            "Epoch 102/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.5770 - accuracy: 0.3456 - val_loss: 2.7629 - val_accuracy: 0.3168\n",
            "Epoch 103/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.5727 - accuracy: 0.3434 - val_loss: 2.7678 - val_accuracy: 0.2904\n",
            "Epoch 104/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.5467 - accuracy: 0.3452 - val_loss: 2.7525 - val_accuracy: 0.3168\n",
            "Epoch 105/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.5550 - accuracy: 0.3471 - val_loss: 2.7366 - val_accuracy: 0.3201\n",
            "Epoch 106/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.5458 - accuracy: 0.3507 - val_loss: 2.7334 - val_accuracy: 0.3333\n",
            "Epoch 107/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.5388 - accuracy: 0.3482 - val_loss: 2.7678 - val_accuracy: 0.3366\n",
            "Epoch 108/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.5561 - accuracy: 0.3485 - val_loss: 2.7139 - val_accuracy: 0.3168\n",
            "Epoch 109/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.5294 - accuracy: 0.3529 - val_loss: 2.7380 - val_accuracy: 0.3168\n",
            "Epoch 110/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.5165 - accuracy: 0.3551 - val_loss: 2.7006 - val_accuracy: 0.3267\n",
            "Epoch 111/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.5216 - accuracy: 0.3581 - val_loss: 2.7414 - val_accuracy: 0.3069\n",
            "Epoch 112/500\n",
            "85/85 [==============================] - 5s 53ms/step - loss: 2.5071 - accuracy: 0.3607 - val_loss: 2.7152 - val_accuracy: 0.3201\n",
            "Epoch 113/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.4951 - accuracy: 0.3632 - val_loss: 2.7181 - val_accuracy: 0.3234\n",
            "Epoch 114/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.5067 - accuracy: 0.3621 - val_loss: 2.7197 - val_accuracy: 0.3234\n",
            "Epoch 115/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.5053 - accuracy: 0.3585 - val_loss: 2.6910 - val_accuracy: 0.3300\n",
            "Epoch 116/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.4981 - accuracy: 0.3496 - val_loss: 2.7054 - val_accuracy: 0.3234\n",
            "Epoch 117/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.4811 - accuracy: 0.3669 - val_loss: 2.7143 - val_accuracy: 0.3300\n",
            "Epoch 118/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.4774 - accuracy: 0.3636 - val_loss: 2.7172 - val_accuracy: 0.3366\n",
            "Epoch 119/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.4754 - accuracy: 0.3688 - val_loss: 2.6684 - val_accuracy: 0.3366\n",
            "Epoch 120/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.4740 - accuracy: 0.3699 - val_loss: 2.6761 - val_accuracy: 0.3300\n",
            "Epoch 121/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.4652 - accuracy: 0.3691 - val_loss: 2.6681 - val_accuracy: 0.3399\n",
            "Epoch 122/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.4591 - accuracy: 0.3651 - val_loss: 2.6525 - val_accuracy: 0.3399\n",
            "Epoch 123/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.4458 - accuracy: 0.3717 - val_loss: 2.7115 - val_accuracy: 0.3366\n",
            "Epoch 124/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.4506 - accuracy: 0.3695 - val_loss: 2.6918 - val_accuracy: 0.3399\n",
            "Epoch 125/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.4528 - accuracy: 0.3695 - val_loss: 2.6737 - val_accuracy: 0.3564\n",
            "Epoch 126/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.4413 - accuracy: 0.3684 - val_loss: 2.6603 - val_accuracy: 0.3366\n",
            "Epoch 127/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.4409 - accuracy: 0.3735 - val_loss: 2.6814 - val_accuracy: 0.3201\n",
            "Epoch 128/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.4228 - accuracy: 0.3750 - val_loss: 2.6318 - val_accuracy: 0.3597\n",
            "Epoch 129/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.4289 - accuracy: 0.3794 - val_loss: 2.6634 - val_accuracy: 0.3300\n",
            "Epoch 130/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.4202 - accuracy: 0.3849 - val_loss: 2.6469 - val_accuracy: 0.3432\n",
            "Epoch 131/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.4097 - accuracy: 0.3842 - val_loss: 2.6456 - val_accuracy: 0.3498\n",
            "Epoch 132/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.4006 - accuracy: 0.3717 - val_loss: 2.6360 - val_accuracy: 0.3366\n",
            "Epoch 133/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.4054 - accuracy: 0.3838 - val_loss: 2.6223 - val_accuracy: 0.3498\n",
            "Epoch 134/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3939 - accuracy: 0.3912 - val_loss: 2.6458 - val_accuracy: 0.3564\n",
            "Epoch 135/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.3915 - accuracy: 0.3779 - val_loss: 2.6304 - val_accuracy: 0.3564\n",
            "Epoch 136/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.3795 - accuracy: 0.3846 - val_loss: 2.6274 - val_accuracy: 0.3498\n",
            "Epoch 137/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.3758 - accuracy: 0.3971 - val_loss: 2.6265 - val_accuracy: 0.3366\n",
            "Epoch 138/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3824 - accuracy: 0.3842 - val_loss: 2.7131 - val_accuracy: 0.3168\n",
            "Epoch 139/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.3936 - accuracy: 0.3809 - val_loss: 2.6560 - val_accuracy: 0.3465\n",
            "Epoch 140/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3769 - accuracy: 0.3824 - val_loss: 2.6130 - val_accuracy: 0.3498\n",
            "Epoch 141/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.3608 - accuracy: 0.3945 - val_loss: 2.6076 - val_accuracy: 0.3663\n",
            "Epoch 142/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3682 - accuracy: 0.3949 - val_loss: 2.5942 - val_accuracy: 0.3630\n",
            "Epoch 143/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.3628 - accuracy: 0.3938 - val_loss: 2.6033 - val_accuracy: 0.3465\n",
            "Epoch 144/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.3686 - accuracy: 0.3919 - val_loss: 2.6263 - val_accuracy: 0.3333\n",
            "Epoch 145/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3396 - accuracy: 0.3985 - val_loss: 2.5853 - val_accuracy: 0.3531\n",
            "Epoch 146/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3538 - accuracy: 0.3938 - val_loss: 2.6102 - val_accuracy: 0.3333\n",
            "Epoch 147/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.3448 - accuracy: 0.3941 - val_loss: 2.6586 - val_accuracy: 0.3300\n",
            "Epoch 148/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3418 - accuracy: 0.3926 - val_loss: 2.5839 - val_accuracy: 0.3498\n",
            "Epoch 149/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.3208 - accuracy: 0.3923 - val_loss: 2.5809 - val_accuracy: 0.3729\n",
            "Epoch 150/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3340 - accuracy: 0.3982 - val_loss: 2.5974 - val_accuracy: 0.3498\n",
            "Epoch 151/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3131 - accuracy: 0.4051 - val_loss: 2.5847 - val_accuracy: 0.3531\n",
            "Epoch 152/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.3178 - accuracy: 0.4018 - val_loss: 2.5957 - val_accuracy: 0.3663\n",
            "Epoch 153/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.3189 - accuracy: 0.4037 - val_loss: 2.5661 - val_accuracy: 0.3696\n",
            "Epoch 154/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.3048 - accuracy: 0.4099 - val_loss: 2.5799 - val_accuracy: 0.3597\n",
            "Epoch 155/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.2986 - accuracy: 0.4051 - val_loss: 2.5861 - val_accuracy: 0.3762\n",
            "Epoch 156/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.3044 - accuracy: 0.4077 - val_loss: 2.5552 - val_accuracy: 0.3663\n",
            "Epoch 157/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.2808 - accuracy: 0.4110 - val_loss: 2.5607 - val_accuracy: 0.3630\n",
            "Epoch 158/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.2902 - accuracy: 0.4154 - val_loss: 2.5637 - val_accuracy: 0.3564\n",
            "Epoch 159/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.2835 - accuracy: 0.4062 - val_loss: 2.5803 - val_accuracy: 0.3597\n",
            "Epoch 160/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.2785 - accuracy: 0.4140 - val_loss: 2.5264 - val_accuracy: 0.3729\n",
            "Epoch 161/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.2788 - accuracy: 0.4143 - val_loss: 2.5336 - val_accuracy: 0.3729\n",
            "Epoch 162/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.2739 - accuracy: 0.4132 - val_loss: 2.5314 - val_accuracy: 0.3663\n",
            "Epoch 163/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.2600 - accuracy: 0.4169 - val_loss: 2.5508 - val_accuracy: 0.3696\n",
            "Epoch 164/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.2678 - accuracy: 0.4162 - val_loss: 2.5186 - val_accuracy: 0.3696\n",
            "Epoch 165/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.2600 - accuracy: 0.4154 - val_loss: 2.5435 - val_accuracy: 0.3762\n",
            "Epoch 166/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.2704 - accuracy: 0.4099 - val_loss: 2.5303 - val_accuracy: 0.3795\n",
            "Epoch 167/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.2519 - accuracy: 0.4154 - val_loss: 2.5403 - val_accuracy: 0.3564\n",
            "Epoch 168/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.2487 - accuracy: 0.4195 - val_loss: 2.5267 - val_accuracy: 0.3795\n",
            "Epoch 169/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.2462 - accuracy: 0.4176 - val_loss: 2.5515 - val_accuracy: 0.3630\n",
            "Epoch 170/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.2389 - accuracy: 0.4217 - val_loss: 2.6038 - val_accuracy: 0.3564\n",
            "Epoch 171/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.2595 - accuracy: 0.4136 - val_loss: 2.5249 - val_accuracy: 0.3762\n",
            "Epoch 172/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.2283 - accuracy: 0.4213 - val_loss: 2.4933 - val_accuracy: 0.3630\n",
            "Epoch 173/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.2293 - accuracy: 0.4265 - val_loss: 2.5088 - val_accuracy: 0.3795\n",
            "Epoch 174/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.2200 - accuracy: 0.4210 - val_loss: 2.5299 - val_accuracy: 0.3861\n",
            "Epoch 175/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.2163 - accuracy: 0.4268 - val_loss: 2.4871 - val_accuracy: 0.3696\n",
            "Epoch 176/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.2140 - accuracy: 0.4320 - val_loss: 2.5179 - val_accuracy: 0.3597\n",
            "Epoch 177/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.2109 - accuracy: 0.4235 - val_loss: 2.5077 - val_accuracy: 0.3696\n",
            "Epoch 178/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.2151 - accuracy: 0.4272 - val_loss: 2.5241 - val_accuracy: 0.3630\n",
            "Epoch 179/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.1980 - accuracy: 0.4199 - val_loss: 2.4900 - val_accuracy: 0.3696\n",
            "Epoch 180/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1922 - accuracy: 0.4257 - val_loss: 2.5089 - val_accuracy: 0.3729\n",
            "Epoch 181/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.2032 - accuracy: 0.4261 - val_loss: 2.4773 - val_accuracy: 0.3861\n",
            "Epoch 182/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.1926 - accuracy: 0.4272 - val_loss: 2.5128 - val_accuracy: 0.3795\n",
            "Epoch 183/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.1803 - accuracy: 0.4393 - val_loss: 2.4754 - val_accuracy: 0.3894\n",
            "Epoch 184/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1876 - accuracy: 0.4254 - val_loss: 2.4822 - val_accuracy: 0.3630\n",
            "Epoch 185/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.1805 - accuracy: 0.4393 - val_loss: 2.4522 - val_accuracy: 0.3828\n",
            "Epoch 186/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1843 - accuracy: 0.4386 - val_loss: 2.4945 - val_accuracy: 0.3696\n",
            "Epoch 187/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.1708 - accuracy: 0.4320 - val_loss: 2.4907 - val_accuracy: 0.3861\n",
            "Epoch 188/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1656 - accuracy: 0.4386 - val_loss: 2.4651 - val_accuracy: 0.3828\n",
            "Epoch 189/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1754 - accuracy: 0.4390 - val_loss: 2.5204 - val_accuracy: 0.3696\n",
            "Epoch 190/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1740 - accuracy: 0.4393 - val_loss: 2.4950 - val_accuracy: 0.3729\n",
            "Epoch 191/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1607 - accuracy: 0.4460 - val_loss: 2.4785 - val_accuracy: 0.3630\n",
            "Epoch 192/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.1416 - accuracy: 0.4441 - val_loss: 2.4771 - val_accuracy: 0.3894\n",
            "Epoch 193/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1499 - accuracy: 0.4456 - val_loss: 2.5218 - val_accuracy: 0.3465\n",
            "Epoch 194/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.1442 - accuracy: 0.4474 - val_loss: 2.4903 - val_accuracy: 0.3729\n",
            "Epoch 195/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1405 - accuracy: 0.4449 - val_loss: 2.4596 - val_accuracy: 0.3828\n",
            "Epoch 196/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.1452 - accuracy: 0.4430 - val_loss: 2.4550 - val_accuracy: 0.3762\n",
            "Epoch 197/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1310 - accuracy: 0.4430 - val_loss: 2.4389 - val_accuracy: 0.3861\n",
            "Epoch 198/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1313 - accuracy: 0.4445 - val_loss: 2.4537 - val_accuracy: 0.3762\n",
            "Epoch 199/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1435 - accuracy: 0.4460 - val_loss: 2.4344 - val_accuracy: 0.3894\n",
            "Epoch 200/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1136 - accuracy: 0.4529 - val_loss: 2.4533 - val_accuracy: 0.3894\n",
            "Epoch 201/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1309 - accuracy: 0.4408 - val_loss: 2.4428 - val_accuracy: 0.3630\n",
            "Epoch 202/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1297 - accuracy: 0.4371 - val_loss: 2.4287 - val_accuracy: 0.3927\n",
            "Epoch 203/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1073 - accuracy: 0.4515 - val_loss: 2.4395 - val_accuracy: 0.3762\n",
            "Epoch 204/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.1017 - accuracy: 0.4544 - val_loss: 2.4330 - val_accuracy: 0.4026\n",
            "Epoch 205/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1147 - accuracy: 0.4618 - val_loss: 2.4785 - val_accuracy: 0.3828\n",
            "Epoch 206/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1024 - accuracy: 0.4515 - val_loss: 2.4568 - val_accuracy: 0.3729\n",
            "Epoch 207/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.1013 - accuracy: 0.4522 - val_loss: 2.4109 - val_accuracy: 0.4026\n",
            "Epoch 208/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.0985 - accuracy: 0.4566 - val_loss: 2.4588 - val_accuracy: 0.3696\n",
            "Epoch 209/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.0949 - accuracy: 0.4621 - val_loss: 2.4274 - val_accuracy: 0.3828\n",
            "Epoch 210/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.0988 - accuracy: 0.4482 - val_loss: 2.4668 - val_accuracy: 0.3861\n",
            "Epoch 211/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.0899 - accuracy: 0.4529 - val_loss: 2.4476 - val_accuracy: 0.3927\n",
            "Epoch 212/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0734 - accuracy: 0.4596 - val_loss: 2.4526 - val_accuracy: 0.3630\n",
            "Epoch 213/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0833 - accuracy: 0.4618 - val_loss: 2.4332 - val_accuracy: 0.3762\n",
            "Epoch 214/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.0878 - accuracy: 0.4537 - val_loss: 2.4303 - val_accuracy: 0.3828\n",
            "Epoch 215/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0615 - accuracy: 0.4654 - val_loss: 2.4034 - val_accuracy: 0.3993\n",
            "Epoch 216/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.0715 - accuracy: 0.4676 - val_loss: 2.4356 - val_accuracy: 0.3960\n",
            "Epoch 217/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0751 - accuracy: 0.4544 - val_loss: 2.4193 - val_accuracy: 0.3927\n",
            "Epoch 218/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0675 - accuracy: 0.4563 - val_loss: 2.4026 - val_accuracy: 0.4191\n",
            "Epoch 219/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.0549 - accuracy: 0.4610 - val_loss: 2.4343 - val_accuracy: 0.3795\n",
            "Epoch 220/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 2.0589 - accuracy: 0.4695 - val_loss: 2.4051 - val_accuracy: 0.3861\n",
            "Epoch 221/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.0600 - accuracy: 0.4647 - val_loss: 2.3781 - val_accuracy: 0.3927\n",
            "Epoch 222/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.0576 - accuracy: 0.4658 - val_loss: 2.3860 - val_accuracy: 0.4059\n",
            "Epoch 223/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.0508 - accuracy: 0.4647 - val_loss: 2.3979 - val_accuracy: 0.3927\n",
            "Epoch 224/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.0418 - accuracy: 0.4717 - val_loss: 2.3895 - val_accuracy: 0.4158\n",
            "Epoch 225/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0271 - accuracy: 0.4790 - val_loss: 2.4179 - val_accuracy: 0.3993\n",
            "Epoch 226/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.0425 - accuracy: 0.4717 - val_loss: 2.4177 - val_accuracy: 0.3894\n",
            "Epoch 227/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.0399 - accuracy: 0.4647 - val_loss: 2.3585 - val_accuracy: 0.4092\n",
            "Epoch 228/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0235 - accuracy: 0.4776 - val_loss: 2.3865 - val_accuracy: 0.4092\n",
            "Epoch 229/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.0296 - accuracy: 0.4665 - val_loss: 2.4014 - val_accuracy: 0.3993\n",
            "Epoch 230/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0205 - accuracy: 0.4688 - val_loss: 2.3795 - val_accuracy: 0.3993\n",
            "Epoch 231/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.0263 - accuracy: 0.4724 - val_loss: 2.4143 - val_accuracy: 0.3762\n",
            "Epoch 232/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.0153 - accuracy: 0.4732 - val_loss: 2.3805 - val_accuracy: 0.4125\n",
            "Epoch 233/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 2.0113 - accuracy: 0.4809 - val_loss: 2.3708 - val_accuracy: 0.3960\n",
            "Epoch 234/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 2.0124 - accuracy: 0.4757 - val_loss: 2.3978 - val_accuracy: 0.3795\n",
            "Epoch 235/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0074 - accuracy: 0.4798 - val_loss: 2.3494 - val_accuracy: 0.4191\n",
            "Epoch 236/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0223 - accuracy: 0.4790 - val_loss: 2.3600 - val_accuracy: 0.4257\n",
            "Epoch 237/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 2.0102 - accuracy: 0.4842 - val_loss: 2.3637 - val_accuracy: 0.4026\n",
            "Epoch 238/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9946 - accuracy: 0.4787 - val_loss: 2.4019 - val_accuracy: 0.3795\n",
            "Epoch 239/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9983 - accuracy: 0.4812 - val_loss: 2.3809 - val_accuracy: 0.3993\n",
            "Epoch 240/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 1.9920 - accuracy: 0.4732 - val_loss: 2.3946 - val_accuracy: 0.3861\n",
            "Epoch 241/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9957 - accuracy: 0.4827 - val_loss: 2.3835 - val_accuracy: 0.3861\n",
            "Epoch 242/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9729 - accuracy: 0.4849 - val_loss: 2.3875 - val_accuracy: 0.3993\n",
            "Epoch 243/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9801 - accuracy: 0.4798 - val_loss: 2.3734 - val_accuracy: 0.4026\n",
            "Epoch 244/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9806 - accuracy: 0.4868 - val_loss: 2.3626 - val_accuracy: 0.4059\n",
            "Epoch 245/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9706 - accuracy: 0.4860 - val_loss: 2.3526 - val_accuracy: 0.4092\n",
            "Epoch 246/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.9818 - accuracy: 0.4783 - val_loss: 2.3500 - val_accuracy: 0.4026\n",
            "Epoch 247/500\n",
            "85/85 [==============================] - 5s 54ms/step - loss: 1.9825 - accuracy: 0.4831 - val_loss: 2.3416 - val_accuracy: 0.3960\n",
            "Epoch 248/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9605 - accuracy: 0.4904 - val_loss: 2.3641 - val_accuracy: 0.3927\n",
            "Epoch 249/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9591 - accuracy: 0.4897 - val_loss: 2.3777 - val_accuracy: 0.4059\n",
            "Epoch 250/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9581 - accuracy: 0.4835 - val_loss: 2.3770 - val_accuracy: 0.4059\n",
            "Epoch 251/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9570 - accuracy: 0.4846 - val_loss: 2.3395 - val_accuracy: 0.4158\n",
            "Epoch 252/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9608 - accuracy: 0.4882 - val_loss: 2.3949 - val_accuracy: 0.3894\n",
            "Epoch 253/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9694 - accuracy: 0.4912 - val_loss: 2.3282 - val_accuracy: 0.4026\n",
            "Epoch 254/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9493 - accuracy: 0.4897 - val_loss: 2.3881 - val_accuracy: 0.3828\n",
            "Epoch 255/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9489 - accuracy: 0.4835 - val_loss: 2.3284 - val_accuracy: 0.4191\n",
            "Epoch 256/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9374 - accuracy: 0.5040 - val_loss: 2.3412 - val_accuracy: 0.4125\n",
            "Epoch 257/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9583 - accuracy: 0.4904 - val_loss: 2.3600 - val_accuracy: 0.4191\n",
            "Epoch 258/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9362 - accuracy: 0.4926 - val_loss: 2.3460 - val_accuracy: 0.4158\n",
            "Epoch 259/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9414 - accuracy: 0.4952 - val_loss: 2.3285 - val_accuracy: 0.4026\n",
            "Epoch 260/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9275 - accuracy: 0.5004 - val_loss: 2.3669 - val_accuracy: 0.3927\n",
            "Epoch 261/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.9260 - accuracy: 0.4897 - val_loss: 2.3087 - val_accuracy: 0.4125\n",
            "Epoch 262/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9250 - accuracy: 0.4926 - val_loss: 2.3362 - val_accuracy: 0.4290\n",
            "Epoch 263/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.9254 - accuracy: 0.4971 - val_loss: 2.3280 - val_accuracy: 0.4290\n",
            "Epoch 264/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.9310 - accuracy: 0.4919 - val_loss: 2.3462 - val_accuracy: 0.4092\n",
            "Epoch 265/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9161 - accuracy: 0.4956 - val_loss: 2.3316 - val_accuracy: 0.4059\n",
            "Epoch 266/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9066 - accuracy: 0.4926 - val_loss: 2.3300 - val_accuracy: 0.4026\n",
            "Epoch 267/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.9102 - accuracy: 0.4978 - val_loss: 2.2969 - val_accuracy: 0.4092\n",
            "Epoch 268/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9139 - accuracy: 0.5022 - val_loss: 2.3072 - val_accuracy: 0.4158\n",
            "Epoch 269/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.9068 - accuracy: 0.5029 - val_loss: 2.3216 - val_accuracy: 0.4092\n",
            "Epoch 270/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.8915 - accuracy: 0.5026 - val_loss: 2.2939 - val_accuracy: 0.4389\n",
            "Epoch 271/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8997 - accuracy: 0.5026 - val_loss: 2.3072 - val_accuracy: 0.4224\n",
            "Epoch 272/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8949 - accuracy: 0.5029 - val_loss: 2.3036 - val_accuracy: 0.4191\n",
            "Epoch 273/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.9015 - accuracy: 0.4967 - val_loss: 2.3304 - val_accuracy: 0.4125\n",
            "Epoch 274/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8913 - accuracy: 0.5070 - val_loss: 2.2912 - val_accuracy: 0.4092\n",
            "Epoch 275/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8946 - accuracy: 0.5000 - val_loss: 2.2958 - val_accuracy: 0.4323\n",
            "Epoch 276/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8857 - accuracy: 0.5026 - val_loss: 2.3276 - val_accuracy: 0.4125\n",
            "Epoch 277/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.8797 - accuracy: 0.5018 - val_loss: 2.3229 - val_accuracy: 0.4092\n",
            "Epoch 278/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8786 - accuracy: 0.5107 - val_loss: 2.3206 - val_accuracy: 0.3927\n",
            "Epoch 279/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8818 - accuracy: 0.5004 - val_loss: 2.3149 - val_accuracy: 0.4125\n",
            "Epoch 280/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.8725 - accuracy: 0.5099 - val_loss: 2.3817 - val_accuracy: 0.3960\n",
            "Epoch 281/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.8756 - accuracy: 0.5063 - val_loss: 2.3815 - val_accuracy: 0.3927\n",
            "Epoch 282/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.9042 - accuracy: 0.5103 - val_loss: 2.4395 - val_accuracy: 0.3762\n",
            "Epoch 283/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.8771 - accuracy: 0.5173 - val_loss: 2.3619 - val_accuracy: 0.4059\n",
            "Epoch 284/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8638 - accuracy: 0.5085 - val_loss: 2.3411 - val_accuracy: 0.4026\n",
            "Epoch 285/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.8617 - accuracy: 0.5151 - val_loss: 2.2670 - val_accuracy: 0.4422\n",
            "Epoch 286/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8412 - accuracy: 0.5191 - val_loss: 2.2923 - val_accuracy: 0.4257\n",
            "Epoch 287/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.8456 - accuracy: 0.5132 - val_loss: 2.3270 - val_accuracy: 0.4158\n",
            "Epoch 288/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.8509 - accuracy: 0.5066 - val_loss: 2.2987 - val_accuracy: 0.4026\n",
            "Epoch 289/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8521 - accuracy: 0.5165 - val_loss: 2.3452 - val_accuracy: 0.4059\n",
            "Epoch 290/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.8491 - accuracy: 0.5143 - val_loss: 2.2884 - val_accuracy: 0.4290\n",
            "Epoch 291/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8400 - accuracy: 0.5129 - val_loss: 2.2837 - val_accuracy: 0.4257\n",
            "Epoch 292/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8390 - accuracy: 0.5158 - val_loss: 2.2631 - val_accuracy: 0.4257\n",
            "Epoch 293/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.8392 - accuracy: 0.5232 - val_loss: 2.2659 - val_accuracy: 0.4356\n",
            "Epoch 294/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.8365 - accuracy: 0.5099 - val_loss: 2.2739 - val_accuracy: 0.4191\n",
            "Epoch 295/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8287 - accuracy: 0.5195 - val_loss: 2.2614 - val_accuracy: 0.4422\n",
            "Epoch 296/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8188 - accuracy: 0.5235 - val_loss: 2.2742 - val_accuracy: 0.4191\n",
            "Epoch 297/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.8254 - accuracy: 0.5202 - val_loss: 2.2503 - val_accuracy: 0.4389\n",
            "Epoch 298/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8274 - accuracy: 0.5202 - val_loss: 2.2785 - val_accuracy: 0.4389\n",
            "Epoch 299/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.8264 - accuracy: 0.5110 - val_loss: 2.2587 - val_accuracy: 0.4290\n",
            "Epoch 300/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8263 - accuracy: 0.5191 - val_loss: 2.2798 - val_accuracy: 0.4224\n",
            "Epoch 301/500\n",
            "85/85 [==============================] - 5s 55ms/step - loss: 1.8177 - accuracy: 0.5224 - val_loss: 2.2797 - val_accuracy: 0.3927\n",
            "Epoch 302/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8160 - accuracy: 0.5246 - val_loss: 2.3342 - val_accuracy: 0.3894\n",
            "Epoch 303/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8292 - accuracy: 0.5221 - val_loss: 2.2900 - val_accuracy: 0.4158\n",
            "Epoch 304/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8100 - accuracy: 0.5283 - val_loss: 2.2854 - val_accuracy: 0.4356\n",
            "Epoch 305/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8112 - accuracy: 0.5312 - val_loss: 2.2959 - val_accuracy: 0.4290\n",
            "Epoch 306/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.8090 - accuracy: 0.5290 - val_loss: 2.3082 - val_accuracy: 0.4125\n",
            "Epoch 307/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.8151 - accuracy: 0.5213 - val_loss: 2.2697 - val_accuracy: 0.4389\n",
            "Epoch 308/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.7932 - accuracy: 0.5305 - val_loss: 2.2701 - val_accuracy: 0.4224\n",
            "Epoch 309/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8072 - accuracy: 0.5272 - val_loss: 2.2599 - val_accuracy: 0.4455\n",
            "Epoch 310/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.7876 - accuracy: 0.5338 - val_loss: 2.2877 - val_accuracy: 0.4224\n",
            "Epoch 311/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.7772 - accuracy: 0.5375 - val_loss: 2.2563 - val_accuracy: 0.4422\n",
            "Epoch 312/500\n",
            "85/85 [==============================] - 5s 61ms/step - loss: 1.7952 - accuracy: 0.5294 - val_loss: 2.2464 - val_accuracy: 0.4158\n",
            "Epoch 313/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.8034 - accuracy: 0.5331 - val_loss: 2.2617 - val_accuracy: 0.4257\n",
            "Epoch 314/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.7759 - accuracy: 0.5287 - val_loss: 2.2375 - val_accuracy: 0.4389\n",
            "Epoch 315/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.7815 - accuracy: 0.5316 - val_loss: 2.2464 - val_accuracy: 0.4191\n",
            "Epoch 316/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7761 - accuracy: 0.5309 - val_loss: 2.2562 - val_accuracy: 0.4290\n",
            "Epoch 317/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.7812 - accuracy: 0.5324 - val_loss: 2.2403 - val_accuracy: 0.4422\n",
            "Epoch 318/500\n",
            "85/85 [==============================] - 5s 61ms/step - loss: 1.7743 - accuracy: 0.5283 - val_loss: 2.2919 - val_accuracy: 0.4323\n",
            "Epoch 319/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.7725 - accuracy: 0.5375 - val_loss: 2.2686 - val_accuracy: 0.4356\n",
            "Epoch 320/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.7616 - accuracy: 0.5327 - val_loss: 2.2817 - val_accuracy: 0.4422\n",
            "Epoch 321/500\n",
            "85/85 [==============================] - 5s 61ms/step - loss: 1.7706 - accuracy: 0.5349 - val_loss: 2.2526 - val_accuracy: 0.4158\n",
            "Epoch 322/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.7514 - accuracy: 0.5324 - val_loss: 2.2103 - val_accuracy: 0.4488\n",
            "Epoch 323/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7576 - accuracy: 0.5463 - val_loss: 2.2902 - val_accuracy: 0.4389\n",
            "Epoch 324/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.7550 - accuracy: 0.5338 - val_loss: 2.2383 - val_accuracy: 0.4290\n",
            "Epoch 325/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.7523 - accuracy: 0.5382 - val_loss: 2.2670 - val_accuracy: 0.4356\n",
            "Epoch 326/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.7735 - accuracy: 0.5257 - val_loss: 2.2721 - val_accuracy: 0.4257\n",
            "Epoch 327/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.7496 - accuracy: 0.5382 - val_loss: 2.2481 - val_accuracy: 0.4323\n",
            "Epoch 328/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7481 - accuracy: 0.5397 - val_loss: 2.2552 - val_accuracy: 0.4290\n",
            "Epoch 329/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.7291 - accuracy: 0.5570 - val_loss: 2.2649 - val_accuracy: 0.4257\n",
            "Epoch 330/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.7576 - accuracy: 0.5335 - val_loss: 2.2284 - val_accuracy: 0.4224\n",
            "Epoch 331/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7403 - accuracy: 0.5467 - val_loss: 2.2208 - val_accuracy: 0.4455\n",
            "Epoch 332/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.7466 - accuracy: 0.5426 - val_loss: 2.2330 - val_accuracy: 0.4224\n",
            "Epoch 333/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.7506 - accuracy: 0.5386 - val_loss: 2.2407 - val_accuracy: 0.4389\n",
            "Epoch 334/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7235 - accuracy: 0.5485 - val_loss: 2.2338 - val_accuracy: 0.4323\n",
            "Epoch 335/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7196 - accuracy: 0.5437 - val_loss: 2.2218 - val_accuracy: 0.4422\n",
            "Epoch 336/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.7220 - accuracy: 0.5460 - val_loss: 2.2385 - val_accuracy: 0.4455\n",
            "Epoch 337/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7188 - accuracy: 0.5456 - val_loss: 2.2407 - val_accuracy: 0.4191\n",
            "Epoch 338/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7144 - accuracy: 0.5522 - val_loss: 2.2191 - val_accuracy: 0.4455\n",
            "Epoch 339/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7364 - accuracy: 0.5537 - val_loss: 2.2336 - val_accuracy: 0.4257\n",
            "Epoch 340/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7378 - accuracy: 0.5423 - val_loss: 2.1968 - val_accuracy: 0.4587\n",
            "Epoch 341/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7090 - accuracy: 0.5574 - val_loss: 2.2083 - val_accuracy: 0.4356\n",
            "Epoch 342/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.7136 - accuracy: 0.5511 - val_loss: 2.2444 - val_accuracy: 0.4422\n",
            "Epoch 343/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.7129 - accuracy: 0.5419 - val_loss: 2.2130 - val_accuracy: 0.4587\n",
            "Epoch 344/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.7017 - accuracy: 0.5493 - val_loss: 2.1951 - val_accuracy: 0.4488\n",
            "Epoch 345/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7066 - accuracy: 0.5544 - val_loss: 2.2269 - val_accuracy: 0.4257\n",
            "Epoch 346/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.7080 - accuracy: 0.5482 - val_loss: 2.2307 - val_accuracy: 0.4323\n",
            "Epoch 347/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.7005 - accuracy: 0.5515 - val_loss: 2.2152 - val_accuracy: 0.4389\n",
            "Epoch 348/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.7037 - accuracy: 0.5585 - val_loss: 2.1939 - val_accuracy: 0.4422\n",
            "Epoch 349/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.6951 - accuracy: 0.5570 - val_loss: 2.2260 - val_accuracy: 0.4455\n",
            "Epoch 350/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.6856 - accuracy: 0.5559 - val_loss: 2.2110 - val_accuracy: 0.4356\n",
            "Epoch 351/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.6921 - accuracy: 0.5603 - val_loss: 2.2633 - val_accuracy: 0.4290\n",
            "Epoch 352/500\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.6838 - accuracy: 0.5551 - val_loss: 2.2152 - val_accuracy: 0.4356\n",
            "Epoch 353/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6867 - accuracy: 0.5607 - val_loss: 2.2267 - val_accuracy: 0.4389\n",
            "Epoch 354/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6954 - accuracy: 0.5562 - val_loss: 2.2527 - val_accuracy: 0.4290\n",
            "Epoch 355/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6982 - accuracy: 0.5570 - val_loss: 2.1988 - val_accuracy: 0.4488\n",
            "Epoch 356/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6741 - accuracy: 0.5581 - val_loss: 2.2431 - val_accuracy: 0.4224\n",
            "Epoch 357/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.6874 - accuracy: 0.5537 - val_loss: 2.2958 - val_accuracy: 0.4158\n",
            "Epoch 358/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.6913 - accuracy: 0.5504 - val_loss: 2.2258 - val_accuracy: 0.4455\n",
            "Epoch 359/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6768 - accuracy: 0.5537 - val_loss: 2.2103 - val_accuracy: 0.4521\n",
            "Epoch 360/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.6794 - accuracy: 0.5614 - val_loss: 2.2377 - val_accuracy: 0.4191\n",
            "Epoch 361/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.6690 - accuracy: 0.5562 - val_loss: 2.2225 - val_accuracy: 0.4290\n",
            "Epoch 362/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6625 - accuracy: 0.5559 - val_loss: 2.2468 - val_accuracy: 0.4554\n",
            "Epoch 363/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6688 - accuracy: 0.5614 - val_loss: 2.2142 - val_accuracy: 0.4521\n",
            "Epoch 364/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6948 - accuracy: 0.5493 - val_loss: 2.2513 - val_accuracy: 0.4257\n",
            "Epoch 365/500\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 1.6694 - accuracy: 0.5585 - val_loss: 2.1993 - val_accuracy: 0.4488\n",
            "Epoch 366/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6645 - accuracy: 0.5592 - val_loss: 2.2360 - val_accuracy: 0.4389\n",
            "Epoch 367/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.6524 - accuracy: 0.5607 - val_loss: 2.1922 - val_accuracy: 0.4389\n",
            "Epoch 368/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6561 - accuracy: 0.5654 - val_loss: 2.2002 - val_accuracy: 0.4521\n",
            "Epoch 369/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.6476 - accuracy: 0.5610 - val_loss: 2.2073 - val_accuracy: 0.4389\n",
            "Epoch 370/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6502 - accuracy: 0.5636 - val_loss: 2.1879 - val_accuracy: 0.4422\n",
            "Epoch 371/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.6789 - accuracy: 0.5493 - val_loss: 2.2308 - val_accuracy: 0.4521\n",
            "Epoch 372/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.6454 - accuracy: 0.5665 - val_loss: 2.2056 - val_accuracy: 0.4521\n",
            "Epoch 373/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6472 - accuracy: 0.5607 - val_loss: 2.1620 - val_accuracy: 0.4620\n",
            "Epoch 374/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.6359 - accuracy: 0.5680 - val_loss: 2.1868 - val_accuracy: 0.4521\n",
            "Epoch 375/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.6476 - accuracy: 0.5625 - val_loss: 2.1824 - val_accuracy: 0.4422\n",
            "Epoch 376/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.6493 - accuracy: 0.5684 - val_loss: 2.2106 - val_accuracy: 0.4554\n",
            "Epoch 377/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.6381 - accuracy: 0.5607 - val_loss: 2.1642 - val_accuracy: 0.4620\n",
            "Epoch 378/500\n",
            "85/85 [==============================] - 5s 59ms/step - loss: 1.6379 - accuracy: 0.5654 - val_loss: 2.2833 - val_accuracy: 0.4026\n",
            "Epoch 379/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.6518 - accuracy: 0.5658 - val_loss: 2.1689 - val_accuracy: 0.4620\n",
            "Epoch 380/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.6123 - accuracy: 0.5676 - val_loss: 2.2128 - val_accuracy: 0.4488\n",
            "Epoch 381/500\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 1.6267 - accuracy: 0.5724 - val_loss: 2.1729 - val_accuracy: 0.4356\n",
            "Epoch 382/500\n",
            "85/85 [==============================] - 5s 58ms/step - loss: 1.6346 - accuracy: 0.5688 - val_loss: 2.2095 - val_accuracy: 0.4290\n",
            "Epoch 383/500\n",
            "27/85 [========>.....................] - ETA: 2s - loss: 1.6986 - accuracy: 0.5428"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "#model.fit(X_train, y_train, batch_size=32, epochs=500, validation_split=0.2)\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=500, validation_data=(X_test, y_test))\n",
        "# Evaluate the model\n",
        "#X_test_encoded = label_encoder.transform(y_test)\n",
        "#y_test_encoded = to_categorical(X_test_encoded)\n",
        "#loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "#print(\"Test Loss:\", loss)\n",
        "#print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSN_Du2vzy4Y",
        "outputId": "2d4ffcfb-5f9d-416f-fc1d-5eb509818440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1833, 62, 47, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3B1aK4xz1Ou",
        "outputId": "01237789-e506-42a1-8cf0-7e60015c1104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1833, 999)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G4AQ2yWWbKF",
        "outputId": "bfb9350e-15d8-4fb6-90d0-5d4d00907fb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7331, 62, 47, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4MctmTZ7xGM",
        "outputId": "bc4465d7-be54-4262-d371-db78de49633c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7331, 1680)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "1HMWIgSmPgDp",
        "outputId": "38157847-3069-4f86-d6e9-3205f46a9a5a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b25e05899c9f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ],
      "source": [
        "len(set(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K3rHUm_Mpp3",
        "outputId": "c5ea5edb-cb83-406e-da3e-dea08a33d051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 25ms/step - loss: 1.3935 - accuracy: 0.5504\n",
            "Test Loss: 1.3934863805770874\n",
            "Test Accuracy: 0.5503876209259033\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8-gRodqNJxf",
        "outputId": "f40aeeb7-b469-400c-c04e-c33b488a4719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 40ms/step - loss: 1.3956 - accuracy: 0.5155\n",
            "Test Loss: 1.3955614566802979\n",
            "Test Accuracy: 0.5155038833618164\n"
          ]
        }
      ],
      "source": [
        "from scipy.ndimage import median_filter\n",
        "X_test_median = []\n",
        "for i in range(len(X_test)):\n",
        "  filtered_img = adaptive_median_filter(X_test[i], 7) \n",
        "  X_test_median.append(filtered_img)\n",
        "\n",
        "# Evaluate the model\n",
        "X_test_encoded = label_encoder.transform(y_test)\n",
        "y_test_encoded = to_categorical(X_test_encoded)\n",
        "loss, accuracy = model.evaluate(np.array(X_test_median), y_test_encoded)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si51-E8ANq1T",
        "outputId": "a0ced9f2-36a1-4ee8-d026-460cb21fc01c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 27ms/step - loss: 1.4081 - accuracy: 0.5078\n",
            "Test Loss: 1.4080581665039062\n",
            "Test Accuracy: 0.5077519416809082\n"
          ]
        }
      ],
      "source": [
        "from scipy.ndimage import gaussian_filter\n",
        "X_test_gaussian = []\n",
        "for i in range(len(X_test)):\n",
        "  filtered_img = median_filter(X_test[i], size=3) \n",
        "  X_test_gaussian.append(filtered_img)\n",
        "\n",
        "# Evaluate the model\n",
        "X_test_encoded = label_encoder.transform(y_test)\n",
        "y_test_encoded = to_categorical(X_test_encoded)\n",
        "loss, accuracy = model.evaluate(np.array(X_test_gaussian), y_test_encoded)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFcwjJB6SDOT",
        "outputId": "674b7b18-f462-41d7-c61f-4e927c60111b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 22ms/step - loss: 1.4081 - accuracy: 0.5078\n",
            "Test Loss: 1.4080581665039062\n",
            "Test Accuracy: 0.5077519416809082\n"
          ]
        }
      ],
      "source": [
        "from scipy.ndimage import laplace\n",
        "X_test_laplace = []\n",
        "for i in range(len(X_test)):\n",
        "  filtered_img = median_filter(X_test[i], size=3) \n",
        "  X_test_laplace.append(filtered_img)\n",
        "\n",
        "# Evaluate the model\n",
        "X_test_encoded = label_encoder.transform(y_test)\n",
        "y_test_encoded = to_categorical(X_test_encoded)\n",
        "loss, accuracy = model.evaluate(np.array(X_test_laplace), y_test_encoded)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ue_HhL-5KUdO",
        "outputId": "b739a1b3-cff2-43b8-ff77-28af40023335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f449846a350>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def adaptive_median_filter(image, max_window_size):\n",
        "    filtered_image = np.copy(image)\n",
        "    height, width, depth = image.shape\n",
        "    \n",
        "    for n in range(depth):\n",
        "      for i in range(height):\n",
        "        for j in range(width):\n",
        "            window_size = 3\n",
        "            while window_size <= max_window_size:\n",
        "                window_radius = window_size // 2\n",
        "                window = image[max(i - window_radius, 0):min(i + window_radius + 1, height),\n",
        "                               max(j - window_radius, 0):min(j + window_radius + 1, width),n]\n",
        "                \n",
        "                median = np.median(window)\n",
        "                current_pixel = image[i, j, n]\n",
        "                \n",
        "                if current_pixel < np.min(window)*2 or current_pixel > np.max(window)/2:\n",
        "                    filtered_image[i, j, n] = median\n",
        "                    break\n",
        "                else:\n",
        "                    window_size += 2\n",
        "                    \n",
        "    return filtered_image\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('/content/image.jpg')\n",
        "image = X_test[0]\n",
        "# Apply the adaptive median filter\n",
        "filtered_image = adaptive_median_filter(image, max_window_size=7)\n",
        "\n",
        "# Display the original and filtered images\n",
        "plt.imshow(filtered_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "l2s_HGIWN6jA",
        "outputId": "9851b6e6-7fa4-41ee-be63-cad43787a82a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f45985bef80>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(X_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Mbl4lxYaKx4A",
        "outputId": "682a7d44-1854-4a7e-ac36-afc0d8e0a3f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f44985e3be0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "type(image)\n",
        "plt.imshow(median_filter(image, size=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ek0QazCMa6Y",
        "outputId": "3b31e84b-dff4-40fb-f468-d3d19d352488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "258"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKLhMWiaByMy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy.ndimage import median_filter\n",
        "\n",
        "# Load and preprocess the image\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    #img = img.resize((224, 224))  # Resize to VGG input size\n",
        "    img = np.array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "# Apply median filter to an image\n",
        "def apply_median_filter(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    img = np.array(img)\n",
        "    filtered_img = median_filter(img, size=3)  # Adjust the filter size as needed\n",
        "    return Image.fromarray(filtered_img)\n",
        "\n",
        "# Load VGG16 model with pre-trained weights\n",
        "#base_model = VGG16(weights='imagenet')\n",
        "#vgg_model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)\n",
        "\n",
        "# Load and preprocess the original image\n",
        "image_path = '/content/image.jpg'\n",
        "original_image = load_and_preprocess_image(image_path)\n",
        "\n",
        "# Apply median filter to the image\n",
        "filtered_image = apply_median_filter(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTwBF2JcMMYs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7Ohl5Dwak57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Esyy7p0Fak8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications.resnet import ResNet101, preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the LFW dataset and prepare the data\n",
        "# Assume you have your own code to load the LFW dataset\n",
        "faces= fetch_lfw_people(min_faces_per_person=20 ,color = True, resize = None)\n",
        "X,y = faces.images, faces.target\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,stratify = y)\n",
        "\n",
        "# Preprocess the images\n",
        "#X_train = preprocess_input(X_train)\n",
        "#X_test = preprocess_input(X_test)"
      ],
      "metadata": {
        "id": "7bxbaJJ_ak-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from tensorflow.keras.applications.resnet import ResNet101, preprocess_input\n",
        "# Load pre-trained ResNet50 model\n",
        "#base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "base_model = ResNet101(weights='imagenet', include_top=False, pooling='avg')\n",
        "#base_model = tf.keras.Model(inputs=base_model.input, outputs=base_model.layers[-1].output)\n",
        "\n",
        "\n",
        "# Define a function to extract face embeddings\n",
        "def extract_face_embeddings(images):\n",
        "    preprocessed_images = preprocess_input(images)\n",
        "    embeddings = base_model.predict(preprocessed_images)\n",
        "    return embeddings\n",
        "\n",
        "# Prepare your training dataset\n",
        "# Load face images of known individuals and assign labels\n",
        "\n",
        "# Extract face embeddings for the training dataset\n",
        "train_embeddings = extract_face_embeddings(X_train)\n",
        "train_labels = y_train  # Replace with your training labels\n",
        "\n",
        "# Set the random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Train the k-NN classifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_classifier.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Prepare your testing dataset\n",
        "# Load face images for face recognition\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "DiahP4RbalEE",
        "outputId": "85c7b5b9-721e-446d-a8fb-ae9f202acaea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/85 [==============================] - 207s 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=3)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract face embeddings for the testing dataset\n",
        "test_embeddings = extract_face_embeddings(X_test)\n",
        "\n",
        "# Set the random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Perform face recognition using k-NN\n",
        "predicted_labels = knn_classifier.predict(test_embeddings)\n",
        "\n",
        "# Evaluate the accuracy of the face recognition system\n",
        "# Compare predicted labels with ground truth labels for testing dataset\n",
        "accuracy = np.mean(predicted_labels == y_test)*100\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzDt8Y7Tal2a",
        "outputId": "77ddb790-a126-4661-ec84-6424d6d44aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 24s 2s/step\n",
            "Accuracy: 0.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications.resnet import ResNet101, preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Load the LFW dataset\n",
        "lfw_dataset = fetch_lfw_people(min_faces_per_person=5, resize=0.1, color = True)\n",
        "X = lfw_dataset.images\n",
        "y = lfw_dataset.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the images and add channel dimension\n",
        "X_train = preprocess_input(X_train)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = preprocess_input(X_test)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_classes = len(lfw_dataset.target_names)\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define the embedding dimension\n",
        "embedding_dim = 128\n",
        "\n",
        "# Create a base model using ResNet101\n",
        "base_model = ResNet101(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Freeze the weights of the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the inputs\n",
        "anchor_input = Input(shape=X_train.shape[1:])\n",
        "positive_input = Input(shape=X_train.shape[1:])\n",
        "negative_input = Input(shape=X_train.shape[1:])\n",
        "\n",
        "# Generate the embeddings\n",
        "anchor_embedding = base_model(anchor_input)\n",
        "positive_embedding = base_model(positive_input)\n",
        "negative_embedding = base_model(negative_input)\n",
        "\n",
        "# Define the triplet loss function\n",
        "def triplet_loss(y_true, y_pred, alpha=0.3):\n",
        "    anchor_embedding, positive_embedding, negative_embedding = y_pred[0], y_pred[1], y_pred[2]\n",
        "    positive_distance = K.sum(K.square(anchor_embedding - positive_embedding), axis=-1)\n",
        "    negative_distance = K.sum(K.square(anchor_embedding - negative_embedding), axis=-1)\n",
        "    return K.sum(K.maximum(positive_distance - negative_distance + alpha, 0.0))\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=[anchor_embedding, positive_embedding, negative_embedding])\n",
        "\n",
        "# Compile the model with the triplet loss\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=triplet_loss)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([X_train, X_train, X_train], y_train, batch_size=32, epochs=10, validation_data=([X_test,X_test,X_test], y_test))\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"face_recognition_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDJ9vpttd3QW",
        "outputId": "b8d77c5f-244a-467f-efc9-e64e1c9949ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "150/150 [==============================] - 34s 102ms/step - loss: 1.0110 - resnet101_loss: 0.3370 - resnet101_1_loss: 0.3370 - resnet101_2_loss: 0.3370 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 9s 58ms/step - loss: 1.0132 - resnet101_loss: 0.3377 - resnet101_1_loss: 0.3377 - resnet101_2_loss: 0.3377 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 7s 49ms/step - loss: 1.0393 - resnet101_loss: 0.3464 - resnet101_1_loss: 0.3464 - resnet101_2_loss: 0.3464 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 9s 58ms/step - loss: 0.9853 - resnet101_loss: 0.3284 - resnet101_1_loss: 0.3284 - resnet101_2_loss: 0.3284 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 8s 55ms/step - loss: 1.0830 - resnet101_loss: 0.3610 - resnet101_1_loss: 0.3610 - resnet101_2_loss: 0.3610 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 7s 49ms/step - loss: 1.0640 - resnet101_loss: 0.3547 - resnet101_1_loss: 0.3547 - resnet101_2_loss: 0.3547 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 9s 59ms/step - loss: 0.9878 - resnet101_loss: 0.3293 - resnet101_1_loss: 0.3293 - resnet101_2_loss: 0.3293 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 9s 58ms/step - loss: 1.0521 - resnet101_loss: 0.3507 - resnet101_1_loss: 0.3507 - resnet101_2_loss: 0.3507 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 8s 56ms/step - loss: 0.9268 - resnet101_loss: 0.3089 - resnet101_1_loss: 0.3089 - resnet101_2_loss: 0.3089 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 7s 47ms/step - loss: 0.9630 - resnet101_loss: 0.3210 - resnet101_1_loss: 0.3210 - resnet101_2_loss: 0.3210 - val_loss: 1.1323 - val_resnet101_loss: 0.3774 - val_resnet101_1_loss: 0.3774 - val_resnet101_2_loss: 0.3774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "_, test_anchor_embeddings, _ = model.predict([X_test, X_test, X_test])\n",
        "\n",
        "# Calculate the distance between anchor and test embeddings\n",
        "distances = np.sum(np.square(test_anchor_embeddings - test_anchor_embeddings[:, np.newaxis]), axis=-1)\n",
        "\n",
        "# Find the indices of the closest embeddings for each anchor embedding\n",
        "closest_indices = np.argmin(distances, axis=-1)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = np.mean(y_test.argmax(axis=-1) == y_test[closest_indices].argmax(axis=-1))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "P5iRRUyEoCTe",
        "outputId": "f5017ff7-64b9-4c5e-992a-cc1ac9ba5c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8944a753f63a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Compile the loaded model (if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtriplet_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications.resnet import ResNet101, preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Load the LFW dataset\n",
        "lfw_dataset = fetch_lfw_people(min_faces_per_person=5, resize=0.1, color = True)\n",
        "X = lfw_dataset.images\n",
        "y = lfw_dataset.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the images and add channel dimension\n",
        "X_train = preprocess_input(X_train)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = preprocess_input(X_test)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_classes = len(lfw_dataset.target_names)\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define the triplet loss function\n",
        "def triplet_loss(y_true, y_pred, alpha=0.3):\n",
        "    anchor_embedding, positive_embedding, negative_embedding = y_pred[0], y_pred[1], y_pred[2]\n",
        "    positive_distance = K.sum(K.square(anchor_embedding - positive_embedding), axis=-1)\n",
        "    negative_distance = K.sum(K.square(anchor_embedding - negative_embedding), axis=-1)\n",
        "    return K.sum(K.maximum(positive_distance - negative_distance + alpha, 0.0))\n",
        "\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model(\"face_recognition_model.h5\", compile=False)\n",
        "\n",
        "# Compile the loaded model (if necessary)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=triplet_loss)\n",
        "\n",
        "X_test = X_test[:100]\n",
        "y_test = y_test[:100]\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "_, test_anchor_embeddings, _ = model.predict([X_test, X_test, X_test])\n",
        "\n",
        "# Calculate the distance between anchor and test embeddings\n",
        "distances = np.sum(np.square(test_anchor_embeddings - test_anchor_embeddings[:, np.newaxis]), axis=-1)\n",
        "\n",
        "# Find the indices of the closest embeddings for each anchor embedding\n",
        "closest_indices = np.argmin(distances, axis=-1)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = np.mean(y_test.argmax(axis=-1) == y_test[closest_indices].argmax(axis=-1))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "Sx0sfpS6ph4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmkwQelXwgE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}